{"cells":[{"cell_type":"markdown","metadata":{},"source":["# MLZoomcamp 2022 - Session #6 - Homework\n","\n","Author: Jos√© Victor"]},{"cell_type":"markdown","metadata":{},"source":["* Dataset: [California Housing Prices](https://www.kaggle.com/datasets/camnugent/california-housing-prices)"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Loading the data\n","\n","Use only the following columns:\n","\n","* `latitude`,\n","* `longitude`,\n","* `housing_median_age`,\n","* `total_rooms`,\n","* `total_bedrooms`,\n","* `population`,\n","* `households`,\n","* `median_income`,\n","* `median_house_value`,\n","* `ocean_proximity`.\n","* Fill NAs with 0.\n","* Apply the log transform to `median_house_value`.\n","* Do train/validation/test split with 60%/20%/20% distribution.\n","* Use the `train_test_split` function and set the `random_state` parameter to 1.\n","* Use `DictVectorizer` to turn the dataframe into matrices. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 1\n","\n","Let's train a decision tree regressor to predict the `median_house_value` variable.\n","\n","* Train a model with `max_depth=1`.\n","\n","Which feature is used for splitting the data?\n","\n","* ( ) `ocean_proximity=INLAND`\n","* ( ) `total_rooms`\n","* ( ) `latitude`\n","* ( ) `population`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 2\n","\n","Train a random forest model with these parameters:\n","\n","* `n_estimators=10`\n","* `random_state=1`\n","* `n_jobs=-1` (optional - to make training faster)\n","\n","What's the RMSE of this model on validation?\n","\n","* ( ) 0.05\n","* ( ) 0.25\n","* ( ) 0.55\n","* ( ) 0.85"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 3\n","\n","Now let's experiment with the `n_estimators` parameter\n","\n","* Try different values of this parameter from 10 to 200 with step 10.\n","* Set `random_state` to `1`.\n","* Evaluate the model on the validation dataset.\n","\n","After which value of `n_estimators` does RMSE stop improving?\n","\n","* ( ) 10\n","* ( ) 55\n","* ( ) 75\n","* ( ) 150"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 4\n","\n","Let's select the best `max_depth`:\n","\n","* Try different values of `max_depth`: `[10, 15, 20, 25]`\n","* For each of these values, try different values of `n_estimators` from 10 till 200 (with step 10)\n","* Fix the random seed: `random_seed=1`\n","\n","What's the best `max_depth`:\n","\n","* ( ) 10\n","* ( ) 15\n","* ( ) 20\n","* ( ) 25"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 5\n","\n","We can extract feature importance information from tree-based models.\n","\n","At each step of the decision tree learning algorithm, it finds the best split. When doing it, we can calculate \"gain\" - the reduction in impurity before and after the split. This gain is quite useful in understanding what are the important features for tree-based models.\n","\n","In Scikit-Learn, tree-based models contain this information in the `feature_importances_` field.\n","\n","For this homework question, we'll find the most important feature:\n","\n","* Train the model with these parameters:\n","    * `n_estimators=10`,\n","    * `max_depth=20`,\n","    * `random_state=1`,\n","    * `n_jobs=-1` (optional)\n","* Get the feature importance information from this model\n","\n","What's the most important feature?\n","\n","* ( ) `total_rooms`\n","* ( ) `median_income`\n","* ( ) `total_bedrooms`\n","* ( ) `longitude`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Question 6\n","\n","Now let's train an XGBoost model! For this question, we'll tune the `eta` parameter:\n","\n","* Install XGBoost\n","* Create DMatrix for train and validation\n","* Create a watchlist\n","* Train a model with these parameters for 100 rounds:\n","\n","```python\n","xgb_params = {\n","    'eta': 0.3,\n","    'max_depth': 6,\n","    'min_child_weight': 1,\n","    'objective': 'reg:squarederror',\n","    'nthread': 8,\n","    'seed': 1,\n","    'verbosity': 1\n","}\n","```\n","\n","Now change `eta` first to `0.1` and then to `0.01`\n","\n","Which eta leads to the best RMSE score on the validation dataset?\n","\n","* ( ) 0.3\n","* ( ) 0.1\n","* ( ) Both gives same"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.10 ('env': venv)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.8.10"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"407e90332109578e0065ba18671dbaca2b32e43f8b825728b86719da6231e0a7"}}},"nbformat":4,"nbformat_minor":2}
